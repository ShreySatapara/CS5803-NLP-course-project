{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93fc8ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import shutil\n",
    "import sys   \n",
    "from glob import glob\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38f73eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import datetime\n",
    "from threading import Lock\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def log(str):\n",
    "    print(str, file=sys.stderr)\n",
    "    sys.stderr.flush()\n",
    "\n",
    "class ResultWriter:\n",
    "    def __init__(self, results_filename):\n",
    "        self.results_filename = results_filename\n",
    "        self.lock = Lock()\n",
    "\n",
    "    def write(self, str):\n",
    "        self.lock.acquire()\n",
    "        try:\n",
    "            with open(self.results_filename +'.txt', \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(str + \"\\n\")\n",
    "        finally:\n",
    "            self.lock.release()\n",
    "\n",
    "    def log(self, msg):\n",
    "        timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]\n",
    "        msg = timestamp + \": \" + msg\n",
    "        log(msg)\n",
    "        self.lock.acquire()\n",
    "        try:\n",
    "            with open(self.results_filename + \".log\", \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(msg + \"\\n\")\n",
    "        finally:\n",
    "            self.lock.release()\n",
    "\n",
    "\n",
    "def get_num_model_parameters(model):\n",
    "    return sum([p.numel() for p in model.parameters() if p.requires_grad])\n",
    "\n",
    "\n",
    "def print_model_parameters(model,rw):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, f'{param:,}'])\n",
    "        total_params += param\n",
    "    rw.log(f'{table}')\n",
    "    rw.log(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "\n",
    "rw = ResultWriter('./logs/CNN_5_10_bert_append_sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2e4aacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df, tokenizer, max_len,ohe):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df\n",
    "        self.ohe = ohe\n",
    "        self.title = df['text']\n",
    "        self.targets = self.ohe.transform(np.array(self.df.label.values).reshape(-1,1)).toarray()\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.title)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if(index < self.__len__()-2):\n",
    "            text1 = str(self.title[index])\n",
    "            text2 = str(self.title[index+1])\n",
    "            text3 = str(self.title[index+2])\n",
    "            target1 = torch.FloatTensor(self.targets[index])\n",
    "            target2 = torch.FloatTensor(self.targets[index+1])\n",
    "            target3 = torch.FloatTensor(self.targets[index+2])\n",
    "        elif(index == self.__len__()-2):\n",
    "            text1 = str(self.title[index])\n",
    "            text2 = str(self.title[index+1])\n",
    "            text3 = ''\n",
    "            target1 = torch.FloatTensor(self.targets[index])\n",
    "            target2 = torch.FloatTensor(self.targets[index+1])\n",
    "            target3 = torch.FloatTensor(self.ohe.transform(np.array(['NONE']).reshape(-1,1)).toarray()[0])\n",
    "        elif(index == self.__len__()-1):\n",
    "            text1 = str(self.title[index])\n",
    "            text2 = ''\n",
    "            text3 = ''\n",
    "            target1 = torch.FloatTensor(self.targets[index])\n",
    "            target2 = torch.FloatTensor(self.ohe.transform(np.array(['NONE']).reshape(-1,1)).toarray()[0])\n",
    "            target3 = torch.FloatTensor(self.ohe.transform(np.array(['NONE']).reshape(-1,1)).toarray()[0])\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text1+'[SEP]'+text2+'[SEP]'+text3,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        \"\"\"input2 = self.tokenizer.encode_plus(\n",
    "            text2,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input3 = self.tokenizer.encode_plus(\n",
    "            text3,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\"\"\"\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n",
    "            'targets': [target1,target2,target3]#torch.FloatTensor(self.targets[index])\n",
    "        }\n",
    "                \n",
    "    \n",
    "    \"\"\"{\n",
    "                'input1':{\n",
    "                    'input_ids': input1['input_ids'].flatten(),\n",
    "                    'attention_mask': input1['attention_mask'].flatten(),\n",
    "                    'token_type_ids': input1[\"token_type_ids\"].flatten(),\n",
    "                    'targets': target1\n",
    "                },\n",
    "                'input2':{\n",
    "                    'input_ids': input2['input_ids'].flatten(),\n",
    "                    'attention_mask': input2['attention_mask'].flatten(),\n",
    "                    'token_type_ids': input2[\"token_type_ids\"].flatten(),\n",
    "                    'targets': target2\n",
    "                },\n",
    "                'input3':{\n",
    "                    'input_ids': input3['input_ids'].flatten(),\n",
    "                    'attention_mask': input3['attention_mask'].flatten(),\n",
    "                    'token_type_ids': input3[\"token_type_ids\"].flatten(),\n",
    "                    'targets': target3\n",
    "                }\n",
    "            }\"\"\"\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6fc189a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self,pre_trained):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.bert_model = AutoModel.from_pretrained(pre_trained)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.linear1 = torch.nn.Linear(768, 13)\n",
    "        self.linear2 = torch.nn.Linear(768, 13)\n",
    "        self.linear3 = torch.nn.Linear(768, 13)\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "    def forward(self, input_ids, attn_mask, token_type_ids):\n",
    "        output = self.bert_model(\n",
    "            input_ids, \n",
    "            attention_mask=attn_mask, \n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        output_dropout = self.dropout(output.pooler_output)\n",
    "        output1 = self.linear1(output_dropout)\n",
    "        output2 = self.linear2(output_dropout)\n",
    "        output3 = self.linear3(output_dropout)\n",
    "        return output1,output2,output3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "796b1f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7703447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(n_epochs, training_loader, validation_loader, model, \n",
    "                optimizer, checkpoint_path, best_model_path):\n",
    "  val_targets = []\n",
    "  val_outputs = []\n",
    "\n",
    "  # initialize tracker for minimum validation loss\n",
    "  valid_loss_min = np.Inf\n",
    "   \n",
    "  for epoch in range(1, n_epochs+1):\n",
    "    train_loss = 0\n",
    "    valid_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    print('# Epoch {}: #'.format(epoch),end='\\t')\n",
    "    for batch_idx, data in enumerate(tqdm(training_loader)):\n",
    "        #print('yyy epoch', batch_idx)\n",
    "        ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        target1 = data['targets'][0].to(device,dtype=torch.float)\n",
    "        target2 = data['targets'][1].to(device,dtype=torch.float)\n",
    "        target3 = data['targets'][2].to(device,dtype=torch.float)\n",
    "        \n",
    "        output1,output2,output3 = model(ids, mask, token_type_ids)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss1 = loss_fn(output1, target1)#focal_loss(outputs, targets)\n",
    "        loss2 = loss_fn(output2, target2)\n",
    "        loss3 = loss_fn(output3, target3)\n",
    "        #print(loss.item())\n",
    "        #optimizer.zero_grad()\n",
    "        #loss1.backward()\n",
    "        #loss2.backward()\n",
    "        #loss3.backward()\n",
    "        loss = loss1+loss2+loss3\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        train_loss = train_loss + ((1 / (batch_idx + 1)) * ((loss1.item()+loss2.item()+loss3.item())/3 - train_loss))\n",
    "        \n",
    "    model.eval()\n",
    "   \n",
    "    with torch.no_grad():\n",
    "      for batch_idx, data in enumerate(tqdm(validation_loader, 0)):\n",
    "            ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            #targets = data['targets'].to(device, dtype = torch.float)\n",
    "            target1 = data['targets'][0].to(device,dtype=torch.float)\n",
    "            target2 = data['targets'][1].to(device,dtype=torch.float)\n",
    "            target3 = data['targets'][2].to(device,dtype=torch.float)\n",
    "        \n",
    "            output1,output2,output3 = model(ids, mask, token_type_ids)\n",
    "\n",
    "            loss1 = loss_fn(output1, target1)#focal_loss(outputs, targets)\n",
    "            loss2 = loss_fn(output2, target2)\n",
    "            loss3 = loss_fn(output3, target3)\n",
    "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * ((loss1.item()+loss2.item()+loss3.item())/3 - valid_loss))\n",
    "            val_targets.extend([target1.cpu().detach().numpy().tolist(),target2.cpu().detach().numpy().tolist(),target3.cpu().detach().numpy().tolist()])\n",
    "            val_outputs.extend([F.softmax(output1,dim=1).cpu().detach().numpy().tolist(),F.softmax(output2,dim=1).cpu().detach().numpy().tolist(),F.softmax(output3,dim=1).cpu().detach().numpy().tolist()])\n",
    "                  \n",
    "      train_loss = train_loss/len(training_loader)\n",
    "      valid_loss = valid_loss/len(validation_loader)\n",
    "      rw.log('epoch:{:.6f} Avg Training Loss: {:.6f} \\tAvg Validation Loss: {:.6f}'.format(int(epoch), train_loss, valid_loss))\n",
    "      \n",
    "      # create checkpoint variable and add important data\n",
    "      checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'valid_loss_min': valid_loss,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()\n",
    "      }\n",
    "        \n",
    "      # save checkpoint\n",
    "      save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n",
    "        \n",
    "      ## TODO: save the model if validation loss has decreased\n",
    "      if valid_loss <= valid_loss_min:\n",
    "        rw.log('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))\n",
    "        # save checkpoint as best model\n",
    "        save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n",
    "        valid_loss_min = valid_loss\n",
    "\n",
    "    print('\\t Done\\n'.format(epoch))\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f58921de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#it'll return true and predicted labels\n",
    "def predict(data_loader,model):\n",
    "    target_list = []\n",
    "    output_list = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(tqdm(data_loader, 0)):\n",
    "          ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "          mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "          token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "          target1 = data['targets'][0].to(device,dtype=torch.float)\n",
    "          target2 = data['targets'][1].to(device,dtype=torch.float)\n",
    "          target3 = data['targets'][2].to(device,dtype=torch.float)\n",
    "          output1,output2,output3 = model(ids, mask, token_type_ids)\n",
    "          target_list.extend([target1.cpu().detach().numpy().tolist(),target2.cpu().detach().numpy().tolist(),target3.cpu().detach().numpy().tolist()])\n",
    "          output_list.extend([F.softmax(output1,dim=1).cpu().detach().numpy().tolist(),F.softmax(output2,dim=1).cpu().detach().numpy().tolist(),F.softmax(output3,dim=1).cpu().detach().numpy().tolist()])\n",
    "    return target_list,output_list#np.array(target_list).argmax(1),np.array(output_list).argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb41df54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ckp(checkpoint_fpath, model, optimizer):\n",
    "    \"\"\"\n",
    "    checkpoint_path: path to save checkpoint\n",
    "    model: model that we want to load checkpoint parameters into       \n",
    "    optimizer: optimizer we defined in previous training\n",
    "    \"\"\"\n",
    "    # load check point\n",
    "    checkpoint = torch.load(checkpoint_fpath)\n",
    "    # initialize state_dict from checkpoint to model\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    # initialize optimizer from checkpoint to optimizer\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    # initialize valid_loss_min from checkpoint to valid_loss_min\n",
    "    valid_loss_min = checkpoint['valid_loss_min']\n",
    "    # return model, optimizer, epoch value, min validation loss \n",
    "    return model, optimizer, checkpoint['epoch'], valid_loss_min\n",
    "\n",
    "def save_ckp(state, is_best, checkpoint_path, best_model_path):\n",
    "    \"\"\"\n",
    "    state: checkpoint we want to save\n",
    "    is_best: is this the best checkpoint; min validation loss\n",
    "    checkpoint_path: path to save checkpoint\n",
    "    best_model_path: path to save best model\n",
    "    \"\"\"\n",
    "    f_path = checkpoint_path\n",
    "    # save checkpoint data to the path given, checkpoint_path\n",
    "    torch.save(state, f_path)\n",
    "    # if it is a best model, min validation loss\n",
    "    if is_best:\n",
    "        best_fpath = best_model_path\n",
    "        # copy that checkpoint file to best path given, best_model_path\n",
    "        shutil.copyfile(f_path, best_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d64a430c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-27 23:37:21.710: pretrained_name: bert-base-uncased\n",
      "2022-04-27 23:37:21.713: MAX_LEN: 256\n",
      "2022-04-27 23:37:21.714: BATCH_SIZE: 8\n",
      "2022-04-27 23:37:21.715: EPOCHS: 5\n",
      "2022-04-27 23:37:21.716: LEARNING RATE: 1e-05\n"
     ]
    }
   ],
   "source": [
    "pretrained_name = \"bert-base-uncased\"\n",
    "\n",
    "# hyperparameters\n",
    "MAX_LEN = 256\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 8\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 1e-05\n",
    "\n",
    "rw.log(f'pretrained_name: {pretrained_name}')\n",
    "rw.log(f'MAX_LEN: {MAX_LEN}')\n",
    "rw.log(f'BATCH_SIZE: {TRAIN_BATCH_SIZE}')\n",
    "rw.log(f'EPOCHS: {EPOCHS}')\n",
    "rw.log(f'LEARNING RATE: {LEARNING_RATE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75e5f077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data\\\\train_data.csv',\n",
       " './data\\\\train_data_append_label.csv',\n",
       " './data\\\\train_data_append_sentence.csv',\n",
       " './data\\\\val_data.csv',\n",
       " './data\\\\val_data_append_sentence.csv']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob('./data/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68f5e1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './data/train_data.csv' \n",
    "test_path = './data/val_data.csv'\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "target_list = list(train_df.label.unique())\n",
    "target_list.sort()\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(np.array(target_list).reshape(-1,1))\n",
    "\n",
    "train_df = train_df.drop(columns=['id','start','end'])\n",
    "test_df = test_df.drop(columns=['id','start','end'])\n",
    "\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7c1e41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "851a6081",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_df, tokenizer, MAX_LEN,ohe)\n",
    "valid_dataset = CustomDataset(val_df, tokenizer, MAX_LEN,ohe)\n",
    "test_dataset = CustomDataset(test_df, tokenizer, MAX_LEN,ohe)\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=0 )\n",
    "val_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=VALID_BATCH_SIZE, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "647f5feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-27 23:37:26.469: device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "rw.log(f'device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b64c71b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-27 23:37:26.488: pretrained name: bert-base-uncased\n",
      "2022-04-27 23:37:26.489: classifier name: multi_output_CE_loss\n",
      "2022-04-27 23:37:26.490: trained weights path: ./trained_weights/bert-base-uncased/multi_output_CE_loss\n"
     ]
    }
   ],
   "source": [
    "classifier_name = 'multi_output_CE_loss'\n",
    "dir_path = \"./trained_weights/\" +pretrained_name+'/'+ classifier_name \n",
    "rw.log(f'pretrained name: {pretrained_name}')\n",
    "rw.log(f'classifier name: {classifier_name}')\n",
    "rw.log(f'trained weights path: {dir_path}')\n",
    "\n",
    "if not os.path.exists(dir_path):  \n",
    "  os.makedirs(dir_path)\n",
    "  print(f\"The new directory is created: {dir_path}\")\n",
    "    \n",
    "ckpt_path = dir_path+\"/current_checkpoint.pt\"\n",
    "best_model_path = dir_path+\"/best_model.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cc38925",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTClass(pretrained_name)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b058f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load Model\n",
    "try:\n",
    "    if(os.path.exists(best_model_path)):\n",
    "        model, optimizer, epoch_val, valid_loss_min = load_ckp(best_model_path, model, optimizer)\n",
    "    elif(os.path.exists(ckpt_path)):\n",
    "        model, optimizer, epoch_val, valid_loss_min = load_ckp(ckpt_model, model, optimizer)\n",
    "except:\n",
    "    print('no model exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70884d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Epoch 1: #\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1927/1927 [05:34<00:00,  5.76it/s]\n",
      "  0%|                                                                                          | 0/482 [00:00<?, ?it/s]C:\\Users\\shrey satapara\\AppData\\Local\\Temp\\ipykernel_69508\\942197752.py:59: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  val_targets.extend([F.softmax(target1).cpu().detach().numpy().tolist(),F.softmax(target2).cpu().detach().numpy().tolist(),F.softmax(target3).cpu().detach().numpy().tolist()])\n",
      "C:\\Users\\shrey satapara\\AppData\\Local\\Temp\\ipykernel_69508\\942197752.py:60: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  val_outputs.extend([F.softmax(output1).cpu().detach().numpy().tolist(),F.softmax(output2).cpu().detach().numpy().tolist(),F.softmax(output3).cpu().detach().numpy().tolist()])\n",
      "100%|███████████████████████████████████████████████████████████████████████████████▋| 480/482 [00:27<00:00, 17.22it/s]C:\\Users\\shrey satapara\\AppData\\Local\\Temp\\ipykernel_69508\\942197752.py:59: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  val_targets.extend([F.softmax(target1).cpu().detach().numpy().tolist(),F.softmax(target2).cpu().detach().numpy().tolist(),F.softmax(target3).cpu().detach().numpy().tolist()])\n",
      "C:\\Users\\shrey satapara\\AppData\\Local\\Temp\\ipykernel_69508\\942197752.py:60: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  val_outputs.extend([F.softmax(output1).cpu().detach().numpy().tolist(),F.softmax(output2).cpu().detach().numpy().tolist(),F.softmax(output3).cpu().detach().numpy().tolist()])\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 482/482 [00:27<00:00, 17.24it/s]\n",
      "2022-04-27 23:43:32.395: epoch:1.000000 Avg Training Loss: 0.000127 \tAvg Validation Loss: 0.000440\n",
      "2022-04-27 23:43:34.282: Validation loss decreased (inf --> 0.000440).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Done\n",
      "\n",
      "# Epoch 2: #\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1927/1927 [05:43<00:00,  5.61it/s]\n",
      "  0%|                                                                                          | 0/482 [00:00<?, ?it/s]C:\\Users\\shrey satapara\\AppData\\Local\\Temp\\ipykernel_69508\\942197752.py:59: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  val_targets.extend([F.softmax(target1).cpu().detach().numpy().tolist(),F.softmax(target2).cpu().detach().numpy().tolist(),F.softmax(target3).cpu().detach().numpy().tolist()])\n",
      "C:\\Users\\shrey satapara\\AppData\\Local\\Temp\\ipykernel_69508\\942197752.py:60: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  val_outputs.extend([F.softmax(output1).cpu().detach().numpy().tolist(),F.softmax(output2).cpu().detach().numpy().tolist(),F.softmax(output3).cpu().detach().numpy().tolist()])\n",
      "100%|███████████████████████████████████████████████████████████████████████████████▋| 480/482 [00:28<00:00, 16.94it/s]C:\\Users\\shrey satapara\\AppData\\Local\\Temp\\ipykernel_69508\\942197752.py:59: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  val_targets.extend([F.softmax(target1).cpu().detach().numpy().tolist(),F.softmax(target2).cpu().detach().numpy().tolist(),F.softmax(target3).cpu().detach().numpy().tolist()])\n",
      "C:\\Users\\shrey satapara\\AppData\\Local\\Temp\\ipykernel_69508\\942197752.py:60: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  val_outputs.extend([F.softmax(output1).cpu().detach().numpy().tolist(),F.softmax(output2).cpu().detach().numpy().tolist(),F.softmax(output3).cpu().detach().numpy().tolist()])\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 482/482 [00:28<00:00, 16.98it/s]\n",
      "2022-04-27 23:49:48.890: epoch:2.000000 Avg Training Loss: 0.000105 \tAvg Validation Loss: 0.000394\n",
      "2022-04-27 23:49:50.917: Validation loss decreased (0.000440 --> 0.000394).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Done\n",
      "\n",
      "# Epoch 3: #\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1927/1927 [05:46<00:00,  5.56it/s]\n",
      "  0%|                                                                                          | 0/482 [00:00<?, ?it/s]C:\\Users\\shrey satapara\\AppData\\Local\\Temp\\ipykernel_69508\\942197752.py:59: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  val_targets.extend([F.softmax(target1).cpu().detach().numpy().tolist(),F.softmax(target2).cpu().detach().numpy().tolist(),F.softmax(target3).cpu().detach().numpy().tolist()])\n",
      "C:\\Users\\shrey satapara\\AppData\\Local\\Temp\\ipykernel_69508\\942197752.py:60: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  val_outputs.extend([F.softmax(output1).cpu().detach().numpy().tolist(),F.softmax(output2).cpu().detach().numpy().tolist(),F.softmax(output3).cpu().detach().numpy().tolist()])\n",
      "100%|███████████████████████████████████████████████████████████████████████████████▋| 480/482 [00:28<00:00, 16.45it/s]C:\\Users\\shrey satapara\\AppData\\Local\\Temp\\ipykernel_69508\\942197752.py:59: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  val_targets.extend([F.softmax(target1).cpu().detach().numpy().tolist(),F.softmax(target2).cpu().detach().numpy().tolist(),F.softmax(target3).cpu().detach().numpy().tolist()])\n",
      "C:\\Users\\shrey satapara\\AppData\\Local\\Temp\\ipykernel_69508\\942197752.py:60: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  val_outputs.extend([F.softmax(output1).cpu().detach().numpy().tolist(),F.softmax(output2).cpu().detach().numpy().tolist(),F.softmax(output3).cpu().detach().numpy().tolist()])\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 482/482 [00:28<00:00, 16.72it/s]\n",
      "2022-04-27 23:56:09.386: epoch:3.000000 Avg Training Loss: 0.000091 \tAvg Validation Loss: 0.000355\n",
      "2022-04-27 23:56:11.922: Validation loss decreased (0.000394 --> 0.000355).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Done\n",
      "\n",
      "# Epoch 4: #\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1927/1927 [05:49<00:00,  5.51it/s]\n",
      "  0%|                                                                                          | 0/482 [00:00<?, ?it/s]C:\\Users\\shrey satapara\\AppData\\Local\\Temp\\ipykernel_69508\\942197752.py:59: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  val_targets.extend([F.softmax(target1).cpu().detach().numpy().tolist(),F.softmax(target2).cpu().detach().numpy().tolist(),F.softmax(target3).cpu().detach().numpy().tolist()])\n",
      "C:\\Users\\shrey satapara\\AppData\\Local\\Temp\\ipykernel_69508\\942197752.py:60: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  val_outputs.extend([F.softmax(output1).cpu().detach().numpy().tolist(),F.softmax(output2).cpu().detach().numpy().tolist(),F.softmax(output3).cpu().detach().numpy().tolist()])\n",
      "100%|███████████████████████████████████████████████████████████████████████████████▋| 480/482 [00:28<00:00, 16.81it/s]C:\\Users\\shrey satapara\\AppData\\Local\\Temp\\ipykernel_69508\\942197752.py:59: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  val_targets.extend([F.softmax(target1).cpu().detach().numpy().tolist(),F.softmax(target2).cpu().detach().numpy().tolist(),F.softmax(target3).cpu().detach().numpy().tolist()])\n",
      "C:\\Users\\shrey satapara\\AppData\\Local\\Temp\\ipykernel_69508\\942197752.py:60: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  val_outputs.extend([F.softmax(output1).cpu().detach().numpy().tolist(),F.softmax(output2).cpu().detach().numpy().tolist(),F.softmax(output3).cpu().detach().numpy().tolist()])\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 482/482 [00:28<00:00, 16.80it/s]\n",
      "2022-04-28 00:02:33.595: epoch:4.000000 Avg Training Loss: 0.000075 \tAvg Validation Loss: 0.000335\n",
      "2022-04-28 00:02:35.592: Validation loss decreased (0.000355 --> 0.000335).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Done\n",
      "\n",
      "# Epoch 5: #\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1927/1927 [05:46<00:00,  5.56it/s]\n",
      "  0%|                                                                                          | 0/482 [00:00<?, ?it/s]C:\\Users\\shrey satapara\\AppData\\Local\\Temp\\ipykernel_69508\\942197752.py:59: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  val_targets.extend([F.softmax(target1).cpu().detach().numpy().tolist(),F.softmax(target2).cpu().detach().numpy().tolist(),F.softmax(target3).cpu().detach().numpy().tolist()])\n",
      "C:\\Users\\shrey satapara\\AppData\\Local\\Temp\\ipykernel_69508\\942197752.py:60: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  val_outputs.extend([F.softmax(output1).cpu().detach().numpy().tolist(),F.softmax(output2).cpu().detach().numpy().tolist(),F.softmax(output3).cpu().detach().numpy().tolist()])\n",
      "100%|███████████████████████████████████████████████████████████████████████████████▋| 480/482 [00:28<00:00, 17.14it/s]C:\\Users\\shrey satapara\\AppData\\Local\\Temp\\ipykernel_69508\\942197752.py:59: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  val_targets.extend([F.softmax(target1).cpu().detach().numpy().tolist(),F.softmax(target2).cpu().detach().numpy().tolist(),F.softmax(target3).cpu().detach().numpy().tolist()])\n",
      "C:\\Users\\shrey satapara\\AppData\\Local\\Temp\\ipykernel_69508\\942197752.py:60: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  val_outputs.extend([F.softmax(output1).cpu().detach().numpy().tolist(),F.softmax(output2).cpu().detach().numpy().tolist(),F.softmax(output3).cpu().detach().numpy().tolist()])\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 482/482 [00:28<00:00, 17.09it/s]\n",
      "2022-04-28 00:08:53.365: epoch:5.000000 Avg Training Loss: 0.000062 \tAvg Validation Loss: 0.000332\n",
      "2022-04-28 00:08:55.335: Validation loss decreased (0.000335 --> 0.000332).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = train_model(EPOCHS, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad26df77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 61/61 [01:30<00:00,  1.48s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 16/16 [00:25<00:00,  1.57s/it]\n"
     ]
    }
   ],
   "source": [
    "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=0)\n",
    "val_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=256, shuffle=False, num_workers=0)\n",
    "train_true,train_pred = predict(train_data_loader,model)\n",
    "val_true,val_pred = predict(val_data_loader,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c38e08ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey satapara\\AppData\\Local\\Temp\\ipykernel_69508\\802007770.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  train_t = np.asarray(train_true)\n"
     ]
    }
   ],
   "source": [
    "train_t = np.asarray(train_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6271b57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_t = list()\n",
    "for i in train_true:\n",
    "    for j in i:\n",
    "        train_t.append(j)\n",
    "train_p = list()\n",
    "for i in train_pred:\n",
    "    for j in i:\n",
    "        train_p.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b845a539",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_t = np.array(train_t).argmax(1)\n",
    "train_p = np.array(train_p).argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d88c54fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 8, 6, ..., 6, 2, 0], dtype=int64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3581ed79",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clf = classification_report(train_t,train_p,zero_division=True,output_dict = True,target_names=target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "455e2b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ANALYSIS': {'precision': 0.7819106501794975,\n",
       "  'recall': 0.9752487562189055,\n",
       "  'f1-score': 0.8679433252158513,\n",
       "  'support': 16080},\n",
       " 'ARG_PETITIONER': {'precision': 0.5373068432671082,\n",
       "  'recall': 0.5445190156599553,\n",
       "  'f1-score': 0.540888888888889,\n",
       "  'support': 2235},\n",
       " 'ARG_RESPONDENT': {'precision': 1.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 1050},\n",
       " 'FAC': {'precision': 0.9113798790544255,\n",
       "  'recall': 0.9692469597754911,\n",
       "  'f1-score': 0.9394231314104381,\n",
       "  'support': 8552},\n",
       " 'ISSUE': {'precision': 0.9121621621621622,\n",
       "  'recall': 0.4326923076923077,\n",
       "  'f1-score': 0.5869565217391305,\n",
       "  'support': 624},\n",
       " 'NONE': {'precision': 0.8407172995780591,\n",
       "  'recall': 0.9056818181818181,\n",
       "  'f1-score': 0.8719912472647703,\n",
       "  'support': 2640},\n",
       " 'PREAMBLE': {'precision': 0.8582788251891753,\n",
       "  'recall': 0.8850681126835075,\n",
       "  'f1-score': 0.8714676390154968,\n",
       "  'support': 7561},\n",
       " 'PRE_NOT_RELIED': {'precision': 1.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 243},\n",
       " 'PRE_RELIED': {'precision': 0.6010685663401603,\n",
       "  'recall': 0.30080213903743314,\n",
       "  'f1-score': 0.40095040095040096,\n",
       "  'support': 2244},\n",
       " 'RATIO': {'precision': 1.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1044},\n",
       " 'RLC': {'precision': 1.0,\n",
       "  'recall': 0.005516154452324665,\n",
       "  'f1-score': 0.010971786833855801,\n",
       "  'support': 1269},\n",
       " 'RPC': {'precision': 0.7830508474576271,\n",
       "  'recall': 0.9375951293759512,\n",
       "  'f1-score': 0.8533825906257215,\n",
       "  'support': 1971},\n",
       " 'STA': {'precision': 0.7095959595959596,\n",
       "  'recall': 0.38705234159779617,\n",
       "  'f1-score': 0.5008912655971479,\n",
       "  'support': 726},\n",
       " 'accuracy': 0.8078029369147257,\n",
       " 'macro avg': {'precision': 0.8411900794480135,\n",
       "  'recall': 0.4879555949750377,\n",
       "  'f1-score': 0.49575898442628474,\n",
       "  'support': 46239},\n",
       " 'weighted avg': {'precision': 0.8187809334645072,\n",
       "  'recall': 0.8078029369147257,\n",
       "  'f1-score': 0.7659371218899996,\n",
       "  'support': 46239}}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8f7058b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_t = list()\n",
    "val_p = list()\n",
    "\n",
    "for i in val_true:\n",
    "    for j in i:\n",
    "        val_t.append(j)\n",
    "\n",
    "for i in val_pred:\n",
    "    for j in i:\n",
    "        val_p.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "766458f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_t = np.array(val_t).argmax(1)\n",
    "val_p = np.array(val_p).argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "36edc694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ANALYSIS': {'precision': 0.6115541723400116,\n",
       "  'recall': 0.7664553876157971,\n",
       "  'f1-score': 0.6802986043492372,\n",
       "  'support': 4102},\n",
       " 'ARG_PETITIONER': {'precision': 0.41156462585034015,\n",
       "  'recall': 0.45318352059925093,\n",
       "  'f1-score': 0.4313725490196078,\n",
       "  'support': 534},\n",
       " 'ARG_RESPONDENT': {'precision': 1.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 243},\n",
       " 'FAC': {'precision': 0.615491651205937,\n",
       "  'recall': 0.6605276256844201,\n",
       "  'f1-score': 0.6372148859543818,\n",
       "  'support': 2009},\n",
       " 'ISSUE': {'precision': 0.8,\n",
       "  'recall': 0.3404255319148936,\n",
       "  'f1-score': 0.47761194029850745,\n",
       "  'support': 141},\n",
       " 'NONE': {'precision': 0.7020725388601037,\n",
       "  'recall': 0.7720797720797721,\n",
       "  'f1-score': 0.7354138398914518,\n",
       "  'support': 702},\n",
       " 'PREAMBLE': {'precision': 0.7194805194805195,\n",
       "  'recall': 0.7351380042462845,\n",
       "  'f1-score': 0.7272249934365975,\n",
       "  'support': 1884},\n",
       " 'PRE_NOT_RELIED': {'precision': 1.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 48},\n",
       " 'PRE_RELIED': {'precision': 0.40458015267175573,\n",
       "  'recall': 0.19413919413919414,\n",
       "  'f1-score': 0.2623762376237624,\n",
       "  'support': 546},\n",
       " 'RATIO': {'precision': 1.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 312},\n",
       " 'RLC': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 357},\n",
       " 'RPC': {'precision': 0.6711864406779661,\n",
       "  'recall': 0.7904191616766467,\n",
       "  'f1-score': 0.7259395050412465,\n",
       "  'support': 501},\n",
       " 'STA': {'precision': 0.5671641791044776,\n",
       "  'recall': 0.20765027322404372,\n",
       "  'f1-score': 0.304,\n",
       "  'support': 183},\n",
       " 'accuracy': 0.6251513578965577,\n",
       " 'macro avg': {'precision': 0.6540841753993162,\n",
       "  'recall': 0.3784629593215617,\n",
       "  'f1-score': 0.3831886581242148,\n",
       "  'support': 11562},\n",
       " 'weighted avg': {'precision': 0.6218652385669596,\n",
       "  'recall': 0.6251513578965577,\n",
       "  'f1-score': 0.5896370287742666,\n",
       "  'support': 11562}}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report(val_t,val_p,zero_division=True,output_dict = True,target_names=target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c646ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clf = classification_report(train_true,train_pred,zero_division=True,output_dict = True,target_names=target_list)\n",
    "val_clf = classification_report(val_true,val_pred,zero_division=True,target_names=target_list,output_dict=True)\n",
    "rw.log(f'{\"_\"*30}Training Results{\"_\"*30}\\n')\n",
    "rw.log(classification_report(train_true,train_pred,zero_division=True,target_names=target_list))\n",
    "rw.log(f'{\"_\"*30}Val Results{\"_\"*30}\\n')\n",
    "rw.log(classification_report(val_true,val_pred,zero_division=True,target_names=target_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cec1643",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 12/12 [00:15<00:00,  1.33s/it]\n",
      "2022-04-25 11:50:54.744: ______________________________Test Results______________________________\n",
      "\n",
      "2022-04-25 11:50:54.749:                 precision    recall  f1-score   support\n",
      "\n",
      "      ANALYSIS       0.71      0.73      0.72       984\n",
      "ARG_PETITIONER       0.33      0.23      0.27        70\n",
      "ARG_RESPONDENT       0.25      0.42      0.31        38\n",
      "           FAC       0.65      0.77      0.71       580\n",
      "         ISSUE       0.63      0.80      0.71        50\n",
      "          NONE       0.82      0.93      0.87       190\n",
      "      PREAMBLE       0.94      0.77      0.85       508\n",
      "PRE_NOT_RELIED       1.00      0.00      0.00        12\n",
      "    PRE_RELIED       0.54      0.46      0.49       142\n",
      "         RATIO       0.32      0.20      0.25        70\n",
      "           RLC       0.46      0.25      0.32       116\n",
      "           RPC       0.72      0.84      0.78        91\n",
      "           STA       0.44      0.61      0.51        28\n",
      "\n",
      "      accuracy                           0.70      2879\n",
      "     macro avg       0.60      0.54      0.52      2879\n",
      "  weighted avg       0.70      0.70      0.69      2879\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=0)\n",
    "test_true,test_pred = predict(test_data_loader,model)\n",
    "test_clf = classification_report(test_true,test_pred,zero_division=True,target_names=target_list,output_dict=True)\n",
    "rw.log(f'{\"_\"*30}Test Results{\"_\"*30}\\n')\n",
    "rw.log(classification_report(test_true,test_pred,zero_division=True,target_names=target_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642d53ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-cuda11_torch] *",
   "language": "python",
   "name": "conda-env-.conda-cuda11_torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
