{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93fc8ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import shutil\n",
    "import sys   \n",
    "from glob import glob\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38f73eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import datetime\n",
    "from threading import Lock\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def log(str):\n",
    "    print(str, file=sys.stderr)\n",
    "    sys.stderr.flush()\n",
    "\n",
    "class ResultWriter:\n",
    "    def __init__(self, results_filename):\n",
    "        self.results_filename = results_filename\n",
    "        self.lock = Lock()\n",
    "\n",
    "    def write(self, str):\n",
    "        self.lock.acquire()\n",
    "        try:\n",
    "            with open(self.results_filename +'.txt', \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(str + \"\\n\")\n",
    "        finally:\n",
    "            self.lock.release()\n",
    "\n",
    "    def log(self, msg):\n",
    "        timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]\n",
    "        msg = timestamp + \": \" + msg\n",
    "        log(msg)\n",
    "        self.lock.acquire()\n",
    "        try:\n",
    "            with open(self.results_filename + \".log\", \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(msg + \"\\n\")\n",
    "        finally:\n",
    "            self.lock.release()\n",
    "\n",
    "\n",
    "def get_num_model_parameters(model):\n",
    "    return sum([p.numel() for p in model.parameters() if p.requires_grad])\n",
    "\n",
    "\n",
    "def print_model_parameters(model,rw):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, f'{param:,}'])\n",
    "        total_params += param\n",
    "    rw.log(f'{table}')\n",
    "    rw.log(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "\n",
    "rw = ResultWriter('./logs/bert_base_uncased_append_sentence_three_append_labels_CE_loss_linear_layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2e4aacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df, tokenizer, max_len,ohe):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df\n",
    "        self.ohe = ohe\n",
    "        self.title = df['text']\n",
    "        self.targets = self.ohe.transform(np.array(self.df.label.values).reshape(-1,1)).toarray()\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.title)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        title = str(self.title[index])\n",
    "        #title = \" \".join(title.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            title,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n",
    "            'targets': torch.FloatTensor(self.targets[index])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6fc189a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self,pre_trained):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.bert_model = AutoModel.from_pretrained(pre_trained)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.linear1 = torch.nn.Linear(768, 13)\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "    def forward(self, input_ids, attn_mask, token_type_ids):\n",
    "        output = self.bert_model(\n",
    "            input_ids, \n",
    "            attention_mask=attn_mask, \n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        output_dropout = self.dropout(self.pool_hidden_state(output))\n",
    "        output = self.linear1(output_dropout)\n",
    "        return output\n",
    "    \n",
    "    def pool_hidden_state(self, last_hidden_state):\n",
    "        last_hidden_state = last_hidden_state[0]\n",
    "        mean_last_hidden_state = torch.mean(last_hidden_state, 1)\n",
    "        return mean_last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e077ee8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalMaxPooling1D(torch.nn.Module):\n",
    "    def __init__(self, data_format='channels_last'):\n",
    "        super(GlobalMaxPooling1D, self).__init__()\n",
    "        self.data_format = data_format\n",
    "        self.step_axis = 1 if self.data_format == 'channels_last' else 2\n",
    "\n",
    "    def forward(self, input):\n",
    "        return torch.max(input, axis=self.step_axis).values\n",
    "\n",
    "class BERTCNN(torch.nn.Module):\n",
    "    def __init__(self,pre_trained):\n",
    "        super(BERTCNN, self).__init__()\n",
    "        self.bert_model = AutoModel.from_pretrained(pre_trained)\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv1d(256, 128, kernel_size=6)\n",
    "        self.conv2 = torch.nn.Conv1d(256, 128, kernel_size=10)\n",
    "        self.mp = GlobalMaxPooling1D('channels_first')\n",
    "        self.bn = torch.nn.BatchNorm1d(128)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.linear1 = torch.nn.Linear(128, 64)\n",
    "        self.linear2 = torch.nn.Linear(128, 64)\n",
    "        self.linear3 = torch.nn.Linear(128,13)\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "    def forward(self, input_ids, attn_mask, token_type_ids):\n",
    "        output = self.bert_model(\n",
    "            input_ids, \n",
    "            attention_mask=attn_mask, \n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        X = self.conv1(output.last_hidden_state)\n",
    "        X = self.mp(X)\n",
    "        #print(X.shape)\n",
    "        X = self.bn(X)\n",
    "        #print(X.shape)\n",
    "        X = F.relu(self.linear1(X))\n",
    "        Y = self.conv2(output.last_hidden_state)\n",
    "        Y = self.mp(Y)\n",
    "        Y = self.bn(Y)\n",
    "        Y = F.relu(self.linear2(Y))\n",
    "        concat = torch.concat((X,Y),dim=1)\n",
    "        output = self.dropout(concat)\n",
    "        output = self.linear3(output)\n",
    "        #output_dropout = self.dropout(output.pooler_output)\n",
    "        #output = self.linear(output_dropout)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "796b1f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7703447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(n_epochs, training_loader, validation_loader, model, \n",
    "                optimizer, checkpoint_path, best_model_path):\n",
    "  val_targets = []\n",
    "  val_outputs = []\n",
    "\n",
    "  # initialize tracker for minimum validation loss\n",
    "  valid_loss_min = np.Inf\n",
    "   \n",
    "  for epoch in range(1, n_epochs+1):\n",
    "    train_loss = 0\n",
    "    valid_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    print('# Epoch {}: #'.format(epoch),end='\\t')\n",
    "    for batch_idx, data in enumerate(tqdm(training_loader)):\n",
    "        #print('yyy epoch', batch_idx)\n",
    "        ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device,dtype=torch.float)\n",
    "        \n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(outputs, targets)#focal_loss(outputs, targets)\n",
    "        #print(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.item() - train_loss))\n",
    "        \n",
    "    model.eval()\n",
    "   \n",
    "    with torch.no_grad():\n",
    "      for batch_idx, data in enumerate(tqdm(validation_loader, 0)):\n",
    "            ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.item() - valid_loss))\n",
    "            val_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            val_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "            \n",
    "      \n",
    "      train_loss = train_loss/len(training_loader)\n",
    "      valid_loss = valid_loss/len(validation_loader)\n",
    "      rw.log('epoch:{:.6f} Avg Training Loss: {:.6f} \\tAvg Validation Loss: {:.6f}'.format(epoch, train_loss, valid_loss))\n",
    "      \n",
    "      # create checkpoint variable and add important data\n",
    "      checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'valid_loss_min': valid_loss,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()\n",
    "      }\n",
    "        \n",
    "      # save checkpoint\n",
    "      save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n",
    "        \n",
    "      ## TODO: save the model if validation loss has decreased\n",
    "      if valid_loss <= valid_loss_min:\n",
    "        rw.log('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))\n",
    "        # save checkpoint as best model\n",
    "        save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n",
    "        valid_loss_min = valid_loss\n",
    "\n",
    "    print('\\t Done\\n'.format(epoch))\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f58921de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#it'll return true and predicted labels\n",
    "def predict(data_loader,model):\n",
    "    target_list = []\n",
    "    output_list = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(tqdm(data_loader, 0)):\n",
    "          ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "          mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "          token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "          targets = data['targets'].to(device, dtype = torch.float)\n",
    "          outputs = model(ids, mask, token_type_ids)\n",
    "          target_list.extend(targets.cpu().detach().numpy().tolist())\n",
    "          output_list.extend(F.softmax(outputs,dim=1).cpu().detach().numpy().tolist())\n",
    "    return np.array(target_list).argmax(1),np.array(output_list).argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb41df54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ckp(checkpoint_fpath, model, optimizer):\n",
    "    \"\"\"\n",
    "    checkpoint_path: path to save checkpoint\n",
    "    model: model that we want to load checkpoint parameters into       \n",
    "    optimizer: optimizer we defined in previous training\n",
    "    \"\"\"\n",
    "    # load check point\n",
    "    checkpoint = torch.load(checkpoint_fpath)\n",
    "    # initialize state_dict from checkpoint to model\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    # initialize optimizer from checkpoint to optimizer\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    # initialize valid_loss_min from checkpoint to valid_loss_min\n",
    "    valid_loss_min = checkpoint['valid_loss_min']\n",
    "    # return model, optimizer, epoch value, min validation loss \n",
    "    return model, optimizer, checkpoint['epoch'], valid_loss_min\n",
    "\n",
    "def save_ckp(state, is_best, checkpoint_path, best_model_path):\n",
    "    \"\"\"\n",
    "    state: checkpoint we want to save\n",
    "    is_best: is this the best checkpoint; min validation loss\n",
    "    checkpoint_path: path to save checkpoint\n",
    "    best_model_path: path to save best model\n",
    "    \"\"\"\n",
    "    f_path = checkpoint_path\n",
    "    # save checkpoint data to the path given, checkpoint_path\n",
    "    torch.save(state, f_path)\n",
    "    # if it is a best model, min validation loss\n",
    "    if is_best:\n",
    "        best_fpath = best_model_path\n",
    "        # copy that checkpoint file to best path given, best_model_path\n",
    "        shutil.copyfile(f_path, best_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d64a430c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 05:53:13.672: pretrained_name: bert-base-uncased\n",
      "2022-05-02 05:53:13.675: MAX_LEN: 256\n",
      "2022-05-02 05:53:13.676: BATCH_SIZE: 8\n",
      "2022-05-02 05:53:13.677: EPOCHS: 5\n",
      "2022-05-02 05:53:13.678: LEARNING RATE: 1e-05\n"
     ]
    }
   ],
   "source": [
    "pretrained_name = \"bert-base-uncased\"\n",
    "\n",
    "# hyperparameters\n",
    "MAX_LEN = 256\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 8\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 1e-05\n",
    "\n",
    "rw.log(f'pretrained_name: {pretrained_name}')\n",
    "rw.log(f'MAX_LEN: {MAX_LEN}')\n",
    "rw.log(f'BATCH_SIZE: {TRAIN_BATCH_SIZE}')\n",
    "rw.log(f'EPOCHS: {EPOCHS}')\n",
    "rw.log(f'LEARNING RATE: {LEARNING_RATE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75e5f077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data\\\\train_data.csv',\n",
       " './data\\\\train_data_append_label.csv',\n",
       " './data\\\\train_data_append_sentence.csv',\n",
       " './data\\\\train_data_append_sentence_four_append_labels.csv',\n",
       " './data\\\\train_data_append_sentence_new_preprocess.csv',\n",
       " './data\\\\train_data_append_sentence_three.csv',\n",
       " './data\\\\train_data_append_sentence_three_append_labels.csv',\n",
       " './data\\\\val_data.csv',\n",
       " './data\\\\val_data_append_sentence.csv',\n",
       " './data\\\\val_data_append_sentence_new_preprocess.csv',\n",
       " './data\\\\val_data_append_sentence_three.csv']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = glob('./data/*.csv')\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68f5e1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = f[6]#'./data/train_data_append_sentence_three.csv' \n",
    "test_path = f[8]#'./data/val_data_append_sentence.csv'\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "target_list = list(train_df.label.unique())\n",
    "target_list.sort()\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(np.array(target_list).reshape(-1,1))\n",
    "\n",
    "train_df = train_df.drop(columns=['id','start','end'])\n",
    "test_df = test_df.drop(columns=['id','start','end'])\n",
    "\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7c1e41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "851a6081",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_df, tokenizer, MAX_LEN,ohe)\n",
    "valid_dataset = CustomDataset(val_df, tokenizer, MAX_LEN,ohe)\n",
    "#test_dataset = CustomDataset(test_df, tokenizer, MAX_LEN,ohe)\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=VALID_BATCH_SIZE, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "647f5feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 05:53:17.854: device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "rw.log(f'device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b64c71b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 05:53:17.861: pretrained name: bert-base-uncased\n",
      "2022-05-02 05:53:17.862: classifier name: linear_layer_append_sentences_three_append_labels_CE_loss\n",
      "2022-05-02 05:53:17.863: trained weights path: ./trained_weights/bert-base-uncased/linear_layer_append_sentences_three_append_labels_CE_loss\n"
     ]
    }
   ],
   "source": [
    "classifier_name = 'linear_layer_append_sentences_three_append_labels_CE_loss'\n",
    "dir_path = \"./trained_weights/\" +pretrained_name+'/'+ classifier_name \n",
    "rw.log(f'pretrained name: {pretrained_name}')\n",
    "rw.log(f'classifier name: {classifier_name}')\n",
    "rw.log(f'trained weights path: {dir_path}')\n",
    "\n",
    "if not os.path.exists(dir_path):  \n",
    "  os.makedirs(dir_path)\n",
    "  print(f\"The new directory is created: {dir_path}\")\n",
    "    \n",
    "ckpt_path = dir_path+\"/current_checkpoint.pt\"\n",
    "best_model_path = dir_path+\"/best_model.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7cc38925",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTCNN(pretrained_name)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b058f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no model exist\n"
     ]
    }
   ],
   "source": [
    "##Load Model\n",
    "try:\n",
    "    if(os.path.exists(best_model_path)):\n",
    "        model, optimizer, epoch_val, valid_loss_min = load_ckp(best_model_path, model, optimizer)\n",
    "    elif(os.path.exists(ckpt_path)):\n",
    "        model, optimizer, epoch_val, valid_loss_min = load_ckp(ckpt_path, model, optimizer)\n",
    "except:\n",
    "    print('no model exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70884d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Epoch 1: #\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1927/1927 [05:13<00:00,  6.14it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 482/482 [00:24<00:00, 19.39it/s]\n",
      "2022-05-02 04:15:32.252: epoch:1.000000 Avg Training Loss: 0.000041 \tAvg Validation Loss: 0.000127\n",
      "2022-05-02 04:15:34.120: Validation loss decreased (inf --> 0.000127).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Done\n",
      "\n",
      "# Epoch 2: #\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1927/1927 [05:14<00:00,  6.13it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 482/482 [00:24<00:00, 19.65it/s]\n",
      "2022-05-02 04:21:16.270: epoch:2.000000 Avg Training Loss: 0.000032 \tAvg Validation Loss: 0.000119\n",
      "2022-05-02 04:21:18.177: Validation loss decreased (0.000127 --> 0.000119).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Done\n",
      "\n",
      "# Epoch 3: #\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1927/1927 [05:20<00:00,  6.02it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 482/482 [00:24<00:00, 19.54it/s]\n",
      "2022-05-02 04:27:05.970: epoch:3.000000 Avg Training Loss: 0.000026 \tAvg Validation Loss: 0.000129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Done\n",
      "\n",
      "# Epoch 4: #\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1927/1927 [05:15<00:00,  6.10it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 482/482 [00:24<00:00, 19.51it/s]\n",
      "2022-05-02 04:32:48.408: epoch:4.000000 Avg Training Loss: 0.000022 \tAvg Validation Loss: 0.000114\n",
      "2022-05-02 04:32:50.360: Validation loss decreased (0.000119 --> 0.000114).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Done\n",
      "\n",
      "# Epoch 5: #\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1927/1927 [05:18<00:00,  6.05it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 482/482 [00:24<00:00, 19.41it/s]\n",
      "2022-05-02 04:38:36.307: epoch:5.000000 Avg Training Loss: 0.000019 \tAvg Validation Loss: 0.000124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = train_model(EPOCHS, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad26df77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1927/1927 [01:40<00:00, 19.20it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 482/482 [00:25<00:00, 18.61it/s]\n",
      "2022-05-02 04:40:44.626: ______________________________Training Results______________________________\n",
      "\n",
      "2022-05-02 04:40:44.645:                 precision    recall  f1-score   support\n",
      "\n",
      "      ANALYSIS       0.97      0.98      0.98      5371\n",
      "ARG_PETITIONER       0.80      0.98      0.88       746\n",
      "ARG_RESPONDENT       0.93      0.79      0.85       340\n",
      "           FAC       0.99      0.99      0.99      2829\n",
      "         ISSUE       0.90      0.94      0.92       200\n",
      "          NONE       0.99      0.94      0.96       892\n",
      "      PREAMBLE       0.99      1.00      0.99      2487\n",
      "PRE_NOT_RELIED       0.91      0.13      0.23        77\n",
      "    PRE_RELIED       0.93      0.94      0.94       749\n",
      "         RATIO       0.93      0.76      0.84       369\n",
      "           RLC       0.95      0.95      0.95       436\n",
      "           RPC       0.99      0.96      0.97       681\n",
      "           STA       0.93      0.91      0.92       236\n",
      "\n",
      "      accuracy                           0.96     15413\n",
      "     macro avg       0.94      0.87      0.88     15413\n",
      "  weighted avg       0.96      0.96      0.96     15413\n",
      "\n",
      "2022-05-02 04:40:44.646: ______________________________Val Results______________________________\n",
      "\n",
      "2022-05-02 04:40:44.652:                 precision    recall  f1-score   support\n",
      "\n",
      "      ANALYSIS       0.92      0.93      0.93      1357\n",
      "ARG_PETITIONER       0.74      0.96      0.83       177\n",
      "ARG_RESPONDENT       0.87      0.68      0.77        91\n",
      "           FAC       0.90      0.94      0.92       692\n",
      "         ISSUE       0.84      0.65      0.73        55\n",
      "          NONE       0.94      0.79      0.85       220\n",
      "      PREAMBLE       0.97      1.00      0.99       662\n",
      "PRE_NOT_RELIED       1.00      0.10      0.18        20\n",
      "    PRE_RELIED       0.85      0.85      0.85       181\n",
      "         RATIO       0.74      0.61      0.67        83\n",
      "           RLC       0.77      0.70      0.73       106\n",
      "           RPC       0.90      0.88      0.89       143\n",
      "           STA       0.77      0.66      0.71        67\n",
      "\n",
      "      accuracy                           0.90      3854\n",
      "     macro avg       0.86      0.75      0.77      3854\n",
      "  weighted avg       0.90      0.90      0.90      3854\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=0)\n",
    "#val_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=256, shuffle=False, num_workers=0)\n",
    "train_true,train_pred = predict(train_data_loader,model)\n",
    "val_true,val_pred = predict(val_data_loader,model)\n",
    "train_clf = classification_report(train_true,train_pred,zero_division=True,output_dict = True,target_names=target_list)\n",
    "val_clf = classification_report(val_true,val_pred,zero_division=True,target_names=target_list,output_dict=True)\n",
    "rw.log(f'{\"_\"*30}Training Results{\"_\"*30}\\n')\n",
    "rw.log(classification_report(train_true,train_pred,zero_division=True,target_names=target_list))\n",
    "rw.log(f'{\"_\"*30}Val Results{\"_\"*30}\\n')\n",
    "rw.log(classification_report(val_true,val_pred,zero_division=True,target_names=target_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "642d53ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for BERTCNN:\n\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\", \"bn.weight\", \"bn.bias\", \"bn.running_mean\", \"bn.running_var\", \"linear2.weight\", \"linear2.bias\", \"linear3.weight\", \"linear3.bias\". \n\tsize mismatch for linear1.weight: copying a param with shape torch.Size([13, 768]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for linear1.bias: copying a param with shape torch.Size([13]) from checkpoint, the shape in current model is torch.Size([64]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess\u001b[39m(text):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m model, optimizer, epoch_val, valid_loss_min \u001b[38;5;241m=\u001b[39m \u001b[43mload_ckp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      6\u001b[0m test_true \u001b[38;5;241m=\u001b[39m []\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36mload_ckp\u001b[1;34m(checkpoint_fpath, model, optimizer)\u001b[0m\n\u001b[0;32m      8\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(checkpoint_fpath)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# initialize state_dict from checkpoint to model\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstate_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# initialize optimizer from checkpoint to optimizer\u001b[39;00m\n\u001b[0;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\.conda\\envs\\cuda11_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1482\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1477\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   1478\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1479\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   1481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1482\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1483\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   1484\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for BERTCNN:\n\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\", \"bn.weight\", \"bn.bias\", \"bn.running_mean\", \"bn.running_var\", \"linear2.weight\", \"linear2.bias\", \"linear3.weight\", \"linear3.bias\". \n\tsize mismatch for linear1.weight: copying a param with shape torch.Size([13, 768]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for linear1.bias: copying a param with shape torch.Size([13]) from checkpoint, the shape in current model is torch.Size([64])."
     ]
    }
   ],
   "source": [
    "import re\n",
    "def preprocess(text):\n",
    "    return text.replace('\\r','')\n",
    "model, optimizer, epoch_val, valid_loss_min = load_ckp(ckpt_path, model, optimizer)\n",
    "import json\n",
    "test_true = []\n",
    "test_pred = []\n",
    "model.eval()\n",
    "test_data = json.loads(open('./data/val_data.json').read())\n",
    "for i in test_data:  \n",
    "    index = 0\n",
    "    for j in tqdm(range(len(i['annotations'][0]['result']))): \n",
    "        if(index == 0):\n",
    "            prev_1 = ''\n",
    "            prev_2 = ''\n",
    "            prev_3 = ''\n",
    "            pl_1 = '[UNK]'\n",
    "            pl_2 = '[UNK]'\n",
    "            pl_3 = '[UNK]'            \n",
    "        elif(index==1):\n",
    "            prev_1 = ''\n",
    "            prev_2 = ''\n",
    "            prev_3 = preprocess(i['annotations'][0]['result'][j-1]['value']['text'])\n",
    "            pl_1 = '[UNK]'\n",
    "            pl_2 = '[UNK]'\n",
    "            pl_3 = target_list[test_pred[-1][0]]#i['annotations'][0]['result'][j-1]['value']['labels'][0]\n",
    "        elif(index==2):\n",
    "            prev_1 = ''\n",
    "            prev_2 = preprocess(i['annotations'][0]['result'][j-2]['value']['text'])\n",
    "            prev_3 = preprocess(i['annotations'][0]['result'][j-1]['value']['text'])\n",
    "            pl_1 = '[UNK]'\n",
    "            pl_2 = target_list[test_pred[-2][0]]#i['annotations'][0]['result'][j-2]['value']['labels'][0]\n",
    "            pl_3 = target_list[test_pred[-1][0]]#i['annotations'][0]['result'][j-1]['value']['labels'][0]\n",
    "        else:\n",
    "            prev_1 = preprocess(i['annotations'][0]['result'][j-3]['value']['text'])\n",
    "            prev_2 = preprocess(i['annotations'][0]['result'][j-2]['value']['text'])\n",
    "            prev_3 = preprocess(i['annotations'][0]['result'][j-1]['value']['text'])\n",
    "            pl_1 = target_list[test_pred[-3][0]]#i['annotations'][0]['result'][j-3]['value']['labels'][0]\n",
    "            pl_2 = target_list[test_pred[-2][0]]#i['annotations'][0]['result'][j-2]['value']['labels'][0] \n",
    "            pl_3 = target_list[test_pred[-1][0]]#i['annotations'][0]['result'][j-1]['value']['labels'][0]\n",
    "\n",
    "        a = preprocess(i['annotations'][0]['result'][j]['value']['text'])\n",
    "        if(a!=''):\n",
    "            s = pl_1 + '[SEP]' + pl_2 + '[SEP]' +pl_3 + '[SEP]' + prev_1 + '[SEP]' + prev_2 + '[SEP]'  + prev_3 + '[SEP]' +  a #preprocess(i['annotations'][0]['result'][j]['value']['text'])\n",
    "            text = s\n",
    "            text_encode = tokenizer.encode_plus(\n",
    "                    text,\n",
    "                    None,\n",
    "                    add_special_tokens=True,\n",
    "                    max_length=MAX_LEN,\n",
    "                    padding='max_length',\n",
    "                    return_token_type_ids=True,\n",
    "                    truncation=True,\n",
    "                    return_attention_mask=True,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "            input_ids = text_encode['input_ids'].to(device)\n",
    "            token_type_ids = text_encode['token_type_ids'].to(device)\n",
    "            attention_mask = text_encode['attention_mask'].to(device)\n",
    "            output = model(input_ids, attention_mask, token_type_ids)\n",
    "            test_pred.append(F.softmax(output,dim=1).cpu().detach().numpy().argmax(1))\n",
    "            test_true.append(i['annotations'][0]['result'][j]['value']['labels'][0])\n",
    "        else:\n",
    "            test_pred.append(ohe.transform([['NONE']]).toarray().argmax(1))\n",
    "            test_true.append('NONE')\n",
    "        index+=1\n",
    "        #prev_1 = prev_2\n",
    "        #prev_2 = target_list[torch.sigmoid(output).cpu().detach().numpy().argmax(1)[0]]\n",
    "            \n",
    "        \n",
    "test_pred = np.array(test_pred).reshape(1,-1)[0]    \n",
    "test_true = ohe.transform(np.array(test_true).reshape(-1,1)).toarray().argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2ad458",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clf = classification_report(test_true,test_pred,zero_division=True,target_names=target_list,output_dict=True)\n",
    "rw.log(f'{\"_\"*30}Test Results{\"_\"*30}\\n')\n",
    "rw.log(classification_report(test_true,test_pred,zero_division=True,target_names=target_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c132b1a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-cuda11_torch] *",
   "language": "python",
   "name": "conda-env-.conda-cuda11_torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
